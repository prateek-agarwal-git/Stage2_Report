% \begin{itemize}
%  \item Draw the testbed figure. 
%         Intel Xeon processor (2.2 Ghz, 24 cores)
%         128 GB RAM
%         10G NIC connected via cable (P2P)
%         DPDK installed in Server 2 (v 19.11). 2MB hugepages used.
%         A \& B:	40G NIC, Intel XL710 i40e (QSFP+ cables)
%         C,D,E:     10G Netronome nfp 
%         RAN machine: RAN emulator. DPDK load generator. Same processor as UPF
%         DNN machine: DPDK sink/mirror/load generator. Same processor as UPF
%      
%  \item Describe the testbed for design A/B, design C/D/E. This includes the server configurations (CPU+memory) and connection info, the NIC info (model, bandwidth), DPDK version info, python controller. 
%  \begin{itemize}
%   \item Add the common configuration/setup for all designs first
%   \item Add specific configuration/setup: design A+B, design C+D, design E
%  \end{itemize}
% 
%  \item Parameters and metrics: Describe how our load is generated? What is the duration of our experiments? Describe the traffic distribution used/varied (imix, UL/DL ratios) in a table, and highlight the standard workloads (imix, 1:2 UL:DL) along with appropriate citation. Explain the metrics evaluated (throughput, latency, compute cost, power dissipation).
%  \item Add placeholders for graphs and experiment details in individual subsections (AvsB etc).
% \end{itemize}

We now evaluate our various UPF designs and quantify the performance gains of offloading UPF functionality.

% \begin{figure}[ht]
%  \centering
% \includegraphics[width=0.3\textwidth]{fig/5g_testbed.png}
%  \setlength{\abovecaptionskip}{6pt}
% % \setlength{\belowcaptionskip}{-6pt}
%  \caption{Testbed setup for 5G User Plane Function.}
%  \label{fig:testbed}
% \end{figure}
\noindent \textbf{Experiment Setup.} All our experiments run on three servers with Intel Xeon processors (2.2Ghz, 24 cores) and 128GB RAM. The first server runs a RAN emulator/load generator that generates emulated traffic to the UPF, comprising of control plane PFCP messages and uplink/downlink dataplane traffic from a large number of UEs. The RAN emulator runs over DPDK to generate traffic at a high rate and we ensure that it can generate enough load to saturate the control/data plane of the UPF in all experiments. The second server in our setup runs one of the versions of our UPF. The third server runs a sink application that generates downlink traffic and consumes uplink traffic from the RAN emulator. Our RAN emulator and sink span 12K lines of code. We use two sets of NICs in our experiments. For experiments which evaluate the impact of offloading packet steering, we connect the servers using Intel XL710 i40e 40Gbps NICs that are capable of offloading packet steering. For the rest of the experiments, we use Agilio CX 2x10GbE programmable NICs~\cite{netronome}. 

%Our software UPF code runs on DPDK v19.11 and uses 2MB hugepages. 
\noindent {\textbf{Parameters and metrics.}} 
% We generate different traffic mixes for dataplane experiments by varying the packet size, uplink/downlink traffic ratio, and the percentage of ``oversubscribed'' sessions where the incoming load exceeds the QoS-prescribed rate limit, as shown in Table~\ref{tab:workload-scenarios}. These scenarios also include a typical traffic mix found in real user traffic~\cite{imixLink, 4g-uldl, 5g-uldl}. 
We generate different traffic mixes for dataplane experiments by varying the packet sizes, across $64B$, $1400B$ and a typical traffic mix~(``IMIX'') found in real user traffic~\cite{imixLink}. We use an uplink to downlink traffic ratio of 1:2~\cite{4g-uldl, 5g-uldl}. 
% and the uplink to downlink traffic ratio as $1:2$~\cite{4g-uldl, 5g-uldl}.
% (XXX: Are we including oversubscribed flows?)
Control plane traffic consists of sets of session creation, modification, and deletion PFCP messages processed one after the other---we refer to this set as a control plane ``procedure''. All results reported are for an experiment conducted for 300 seconds, unless mentioned otherwise. We show error-bars that denote the maximum and minimum values where applicable. The performance metrics measured are the dataplane throughput (bps or pps), control plane throughput (procedures/sec), and average end-to-end response latency (RTT, where the response is mirrored by sink for data packets, and is generated by the control plane processing for signaling packets), as measured at UPF saturation. For experiments evaluating the effect of offloading packet steering, we send data from $65K$ concurrent users. For experiments involving dataplane and control plane offload to smartNICs, we emulate $1K$ concurrent users.

%\begin{table}[t]
%\begin{scriptsize}
%\begin{center}
%\def\arraystretch{1.5}%  1 is the default, change whatever you need
%\begin{tabular}{|p{1cm}|p{1.5cm}|p{1.9cm}|p{2.3cm}|}\hline 
%{\bf{Scenario}} & {\bf{Packet size}} & {\bf{Uplink:Downlink}} & {\bf{Oversubscribed flows\%}} \\ \hline 
%{\bf{Typical}} & imix~\cite{imixLink}  & 1:2~\cite{4g-uldl, 5g-uldl} & 1 \\ \hline
%{\bf{T\_1}} &  imix & 1:2 & 25 \\ \hline
%{\bf{T\_2}} & imix  & 1:2 & 0 \\ \hline
%{\bf{T\_3}} & 64B  & 1:2 & 0 \\ \hline
%{\bf{T\_4}} & 1400B  & 1:1 & 0 \\ \hline
%{\bf{T\_XXX}} &   &  &  \\ \hline
%\end{tabular}
%\setlength{\abovecaptionskip}{-6pt}
%% \setlength{\belowcaptionskip}{-10pt}
%\caption{Dataplane workload scenarios.} 
%\label{tab:workload-scenarios}
%\end{center}
%\end{scriptsize}
%\end{table}

% XXX: Please give suitable names for the various designs in the legends and captions. 


