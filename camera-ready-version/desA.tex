\vspace{-3mm}
\subsection{DPDK-based software UPF} 
\label{sub:swUPF}

We begin with describing our purely software DPDK-based UPF (Figure~\ref{fig:all_designs}a) that is representative of the most common UPF design used in production networks today. Our implementation is based on a fully standards-compliant UPF obtained from~\cite{5g-testbed, 5g-testbed-in}. Our UPF supports GTP-based forwarding and AMBR-based QoS enforcement (using a variant of the algorithm in~\cite{carousel}), among other features. Our implementation spans $6.5K$ lines of code. Our UPF has a pipeline-based design, with multiple master and worker threads, each pinned to separate CPU cores. The master threads receive packets from the NIC via the polling-based DPDK APIs, and distribute them to the worker threads for further processing. PFCP packets and dataplane packets are processed by separate worker cores. The inter-core communication between the master and workers is done via the lockless shared rings provided by DPDK, for efficiency and high performance. The worker threads continuously poll the shared rings for received packets, process them, and transmit the output.

When steering packets to worker cores, we would like to ensure that the traffic of a UE is processed by the same worker, in order to avoid splitting the state of a particular UE (forwarding rules, buffered packets, and so on) across multiple workers. This steering is achieved by using the hash over the TCP/IP headers of the``inner'' IP datagram (the datagram originated by the UE, which has been encapsulated within GTP for transit through the core) to partition traffic to worker cores. Note that we cannot simply use a hash over the ``outer'' UDP/IP header fields because dataplane traffic of all UEs between a gNB-UPF pair arrives on the same UDP link, so the outer IP header fields cannot be used to differentiate UEs.  Most modern NICs have the capability to distribute packets to multiple CPU cores via RSS~\cite{rss}; however, the set of header fields used for RSS is restricted to the outer IP header fields. Therefore, a purely software-based DPDK design cannot rely on the NIC to perform packet steering based on the inner IP header fields, and must perform this steering in software.  (RSS based on outer header fields is still useful to distribute traffic to multiple master cores for performance scaling.) Packet steering in software also allows the UPF to efficiently rebalance load across worker cores in case some worker cores are more overloaded than others, and to dynamically scale to more worker cores quickly on demand. In our design, the master cores periodically monitor the queue lengths of the lockless rings shared with the worker cores, and reassigns UEs across workers if it finds that a worker core is overloaded (as inferred from a persistently high queue length), and another underloaded. If all worker cores are overloaded, the master can spawn a worker on new CPU core (when available). We have only implemented a simple load balancing algorithm, but more complex algorithms that assign special classes of UEs (e.g., high priority UEs) to specific worker cores are also possible. 

%While it is recomended that NICs perform RSS over the inner IP fields to distribute packets to worker cores, having capability to perform RSS w.r.t inner IP fields are recommended for distributing packets uniformly across different master cores, but not mandatory in this design. Considering a real-world scenario with multiple RANs so that outer IP fields will have different source addresses, even if the NIC non-uniformly distribute the data packets across master cores based on some RSS hashing, the master cores themselves will re-distribute the packets to worker cores uniformly in the s/w. Thus, this design provides us an option to scale up even with simple NICs.\\

%\noindent \textbf{Implementation details:}
%\begin{itemize}
%    \item The implementation spans $~XX$ lines of code.
%    \item For our 2-NIC setup, we need at least 2 master cores, one polling for packets coming from the RAN and another polling for packets coimg from the DNN. We cannot use only 1 master core it will degrade the performance.
%    \item Although in pipeline design we can process both UL and DL packets in the same worker cores, they are processed in separate worker cores. This decopuling is done to improve performance.
%\end{itemize}
%\noindent \textbf{Advantages:} Doing everything in s/w provides more flexibility in terms of packet distribution or QoS enforcement not available in other designs where some of the computation is offloaded to the h/w.
%\begin{itemize}
%    %\item Since packet distribution happens in s/w, we can use any advanced heuristic instead of simply hashing. For example, we can dedicate all packets from a heavy-hitter UE to only a particular set of worker cores. In future implementations, we can also use heuristics like predicting the load on worker cores beforehand and distributing accordingly.
%    \item Since packet distribution happens in s/w, if a worker core (say, core A) is saturated by some heavy-hitter UEs, we can dynamically balance the load by redirecting packets to other unsaturated worker cores instead of sending them to core A.
%    \item Assuming there are enough master cores, scaling up/down of worker cores is easy and seamless.
%\end{itemize}
%\noindent \textbf{Limitations:}
%\begin{itemize}
%    \item Inter-core communication as well as performing everything in the s/w incurs a computational overhead. As a result, the performance is expected to be less and the latency is expected to be higher than other prototypes which offload some of the computation to the h/w.
%\end{itemize}
